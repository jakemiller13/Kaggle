{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Goal: Using the Kaggle Boston Airbnb dataset, I attempt to use sentiment analysis on written reviews to predict the numerical Airbnb rating. (SPOILER ALERT: It doesn't go very well)**\n",
    "\n",
    "The Airbnb dataset has 3 csv files: \"calendar.csv\", \"listings.csv\", \"reviews.csv\". I first create an LSTM model in Keras and train it on the IMDB sentiment dataset. Then I use the trained model on the comments from \"reviews.csv\" to predict sentiment: either positive (1), or negative (0). Taking the average of these, I see how closely this correlates to the actual ratings, found in \"listings.csv\".\n",
    "\n",
    "A couple things of note: the initial sentiment analysis is a classification task, whereas predicting the rating is really a regression task. My intention is to see how closely we can correlate these.\n",
    "\n",
    "Also, since this is my first Kaggle submission, note that I really like functions. I primarily program in Spyder, so writing functions allows for easy and discrete code chunks, and makes debugging much easier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import string\n",
    "from sklearn.model_selection import train_test_split\n",
    "import keras\n",
    "from keras.models import Sequential, save_model, load_model\n",
    "from keras.layers import Embedding, LSTM, Dense, Flatten\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we load in the IMDB dataset. This is also available via `keras.datasets`, but it comes preprocessed and already encoded there. I want to also work on the preprocessing and cleaning, so I got it directly from the source: http://ai.stanford.edu/~amaas/data/sentiment/ The dataset has 50,000 reviews: 25,000 for the training set and 25,000 for the testing set. Each set is further divided into positive ('pos') and negative ('neg') reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_imdb(directory):\n",
    "    '''\n",
    "    Returns cleaned dataframe of IMDB reviews with columns ['review', 'sentiment']\n",
    "    '''\n",
    "    sentiment = {'neg': 0, 'pos': 1}\n",
    "    df_columns = ['review', 'sentiment']\n",
    "    reviews_with_sentiment = pd.DataFrame(columns = df_columns)\n",
    "    for i in ('test', 'train'):\n",
    "        for j in ('neg', 'pos'):\n",
    "            file_path = directory + i + '/' + j\n",
    "            for file in os.listdir(file_path):\n",
    "                with open((file_path + '/' + file), 'r',\n",
    "                          encoding = 'utf-8') as text_file:\n",
    "                    text = text_file.read()\n",
    "                review = pd.DataFrame([[text, sentiment[j]]],\n",
    "                                      columns = df_columns)\n",
    "                reviews_with_sentiment = reviews_with_sentiment.\\\n",
    "                                         append(review, ignore_index = True)\n",
    "    return reviews_with_sentiment\n",
    "\n",
    "directory = 'Data/IMDB/'\n",
    "cleaned_imdb = clean_imdb(directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at what a single review with sentiment looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "review       Man with the Screaming Brain is a story of gre...\n",
       "sentiment                                                    0\n",
       "Name: 13, dtype: object"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned_imdb.iloc[13]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we will load in one of the GloVe word embeddings (https://nlp.stanford.edu/projects/glove/). A word embedding represents words as vectors, which allows them to actually be interpreted by a computer. There are several different versions of GloVe depending on how many dimensions - and therefore memory - you would like to use. I used one of the smaller ones, with 50 dimensions and 6 billion tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_GloVe(file_path):\n",
    "    '''\n",
    "    Loads word embedding .txt file\n",
    "    Returns word embedding as dictionary\n",
    "    '''\n",
    "    GloVe_dict = dict()\n",
    "    with open(file_path, encoding = 'utf-8') as GloVe_file:\n",
    "        for line in GloVe_file:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            coef = np.asarray(values[1:], dtype = 'float32')\n",
    "            GloVe_dict[word] = coef\n",
    "    return GloVe_dict\n",
    "\n",
    "GloVe_file_path = 'Data/glove.6B.50d.txt'\n",
    "embedding_dict = load_GloVe(GloVe_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check out this word embedding a bit. You can see that it includes 400,000 entries of all lower case words. We'll also look at a random slice of the `embedding_dict` keys."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "400000"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(embedding_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[print(word) for word in list(embedding_dict.keys()) if word != word.lower()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['so',\n",
       " 'them',\n",
       " 'what',\n",
       " 'him',\n",
       " 'united',\n",
       " 'during',\n",
       " 'before',\n",
       " 'may',\n",
       " 'since',\n",
       " 'many',\n",
       " 'while',\n",
       " 'where',\n",
       " 'states',\n",
       " 'because',\n",
       " 'now',\n",
       " 'city',\n",
       " 'made',\n",
       " 'like',\n",
       " 'between',\n",
       " 'did',\n",
       " 'just',\n",
       " 'national',\n",
       " 'day',\n",
       " 'country',\n",
       " 'under']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(embedding_dict.keys())[100:125]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check out how `'under'`, the last word in our list we just printed, is represented in vector space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.3721e-01, -2.9500e-01, -5.9160e-02, -5.9235e-01,  2.3010e-02,\n",
       "        2.1884e-01, -3.4254e-01, -7.0213e-01, -5.5748e-01, -7.8537e-01,\n",
       "        4.6417e-01,  4.4733e-01, -7.4178e-01, -4.6287e-01,  4.2665e-01,\n",
       "        3.9795e-01, -2.1767e-01,  2.6260e-02, -3.1353e-01,  7.8520e-02,\n",
       "        2.8495e-01,  1.1671e-01,  2.9981e-01, -9.1376e-01, -4.7744e-01,\n",
       "       -1.6573e+00,  7.4029e-03, -1.1224e-01, -1.0604e-01,  2.9894e-01,\n",
       "        3.4634e+00, -2.9341e-01, -7.6777e-01, -3.0120e-01, -3.7192e-03,\n",
       "        2.3122e-01,  4.7334e-01,  1.3078e-01,  5.0225e-02,  1.9911e-01,\n",
       "       -5.0179e-01, -3.4197e-03,  3.8654e-01,  5.7375e-02, -1.0157e+00,\n",
       "       -3.3991e-01, -6.1970e-01, -5.9706e-01, -1.1377e-01, -6.4195e-01],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_dict['under']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our reviews with sentiment and our word embedding loaded, it's time to start cleaning up and preprocessing our reviews. The reviews have all sorts of non-alphanumeric and uppercase characters, as shown in this rather eloquent review:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['I was lucky enough to grow up surfing in San Diego (not the biggest waves in the world but it was a hell of childhood, I\\'ll tell you that) and I have seen A LOT of so-called surfer flicks in my life. After watching NORTH SHORE for the first time just now, all I can say is THANK GOD I never saw this as a kid. If I had seen this and mistakenly thought that this was a realistic portrayal of the surf scene, I would sold my board and totally gotten into, I don\\'t know, accounting or something.<br /><br />Seriously, this movie has a as much in common with real surfing as TOP GUN has was real military life. The acting is terrible, the music is worse, the cinematography is iffy at best and OH MY GOD what was Laird Hanilton thinking?! WOW!!! DO NOT SEE THIS MOVIE!!! IT SUCKS!!! If you want a REAL surf flick, see RIDING GIANTS. Hell, watch SURF\\'S UP instead of this. Seriously. Sucks. Sucks bad. Sucks REAL bad. Brah. ;)<br /><br />PS: Had to change my summery from \"WTF?!\" to wtf because, apparently, we are supposed to whisper on this site. NO YELLING!!! (shhhhhh!) ;D',\n",
       "       0], dtype=object)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned_imdb.iloc[489].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will strip off all non-alphanumeric characters. This process takes a while, so I also added a print statement to update us on how far we've gotten. Note that while the `cleaned_imdb` reviews are a `pd.DataFrame`, this function is also used later when we are stripping the Airbnb reviews, which are a `pd.Series`, hence the `if` statement. We first strip off all the punctuation and replace it with a single space. Then we replace all whitespace characters, except actual spaces.\n",
    "\n",
    "It's important to note that this is not always best practice. Consider the following sentence from the review above:\n",
    "\n",
    ">WOW!!! DO NOT SEE THIS MOVIE!!! IT SUCKS!!!\n",
    "\n",
    "You can tell that this carries a much stronger sentiment than just one (or no) exclamation points. There are ways of dealing with punctuation (see [VADER](https://github.com/cjhutto/vaderSentiment) for example), but I did not use them here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stripping review: 0 of 50000\n",
      "Stripping review: 5000 of 50000\n",
      "Stripping review: 10000 of 50000\n",
      "Stripping review: 15000 of 50000\n",
      "Stripping review: 20000 of 50000\n",
      "Stripping review: 25000 of 50000\n",
      "Stripping review: 30000 of 50000\n",
      "Stripping review: 35000 of 50000\n",
      "Stripping review: 40000 of 50000\n",
      "Stripping review: 45000 of 50000\n"
     ]
    }
   ],
   "source": [
    "def strip_punctuation_and_whitespace(reviews_df, verbose = True):\n",
    "    '''\n",
    "    Strips all punctuation and whitespace from reviews EXCEPT spaces (i.e. ' ')\n",
    "    Removes \"<br />\"\n",
    "    Returns dataframe of cleaned IMDB reviews\n",
    "    '''\n",
    "    trans_punc = str.maketrans(string.punctuation,\n",
    "                               ' ' * len(string.punctuation))\n",
    "    whitespace_except_space = string.whitespace.replace(' ', '')\n",
    "    trans_white = str.maketrans(whitespace_except_space,\n",
    "                                ' ' * len(whitespace_except_space))\n",
    "    stripped_df = pd.DataFrame(columns = ['review', 'sentiment'])\n",
    "    for i, row in enumerate(reviews_df.values):\n",
    "        if i % 5000 == 0 and verbose == True:\n",
    "            print('Stripping review: ' + str(i) + ' of ' + str(len(reviews_df)))\n",
    "        if type(reviews_df) == pd.DataFrame:\n",
    "            review = row[0]\n",
    "            sentiment = row[1]\n",
    "        elif type(reviews_df) == pd.Series:\n",
    "            review = row\n",
    "            sentiment = np.NaN\n",
    "        try:\n",
    "            review.replace('<br />', ' ')\n",
    "            for trans in [trans_punc, trans_white]:\n",
    "                review = ' '.join(str(review).translate(trans).split())\n",
    "            combined_df = pd.DataFrame([[review, sentiment]],\n",
    "                                       columns = ['review', 'sentiment'])\n",
    "            stripped_df = pd.concat([stripped_df, combined_df],\n",
    "                                    ignore_index = True)\n",
    "        except AttributeError:\n",
    "            continue\n",
    "    return stripped_df\n",
    "\n",
    "stripped_imdb = strip_punctuation_and_whitespace(cleaned_imdb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
